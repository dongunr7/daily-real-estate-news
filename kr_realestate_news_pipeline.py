# -*- coding: utf-8 -*-
"""
KR Real Estate News Pipeline (macro-focused)
- v31.1 Email Report: 1ì°¨ í•„í„°ë§ í›„ 5ê°œ ë¯¸ë§Œ ì‹œ 2ì°¨ í•„í„°ë§ìœ¼ë¡œ 5ê°œ ì±„ìš°ê¸°
"""

# â”€â”€ gRPC/ABSL ê²½ê³  ì–µì œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import os
os.environ.setdefault("GRPC_VERBOSITY", "ERROR")
os.environ.setdefault("GRPC_TRACE", "none")
os.environ.setdefault("TF_CPP_MIN_LOG_LEVEL", "2")

# í‘œì¤€ ì¶œë ¥ ì¸ì½”ë”© ê³ ì •
import sys
try:
    sys.stdout.reconfigure(encoding="utf-8")
    sys.stderr.reconfigure(encoding="utf-8")
except Exception:
    pass

import re, html, time, requests, traceback, warnings, logging, json
from urllib.parse import urlparse, urlunparse
from datetime import datetime, timedelta
from dateutil import parser as dtparser
import pytz
from dotenv import load_dotenv
from bs4 import BeautifulSoup
import trafilatura
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from concurrent.futures import ThreadPoolExecutor, as_completed

# â”€â”€ (ì¶”ê°€) ì´ë©”ì¼ ë¼ì´ë¸ŒëŸ¬ë¦¬ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import smtplib
from email.mime.multipart import MIMEMultipart
from email.mime.text import MIMEText
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

warnings.filterwarnings("ignore")

# â”€â”€ ë¡œê¹… ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Version / Config
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class Config:
    VERSION = "v31.1 (Email Report, Fill-5)" # ë²„ì „ëª… ìˆ˜ì •
    KST = pytz.timezone("Asia/Seoul")
    NAVER_ENDPOINT = "https://openapi.naver.com/v1/search/news.json"

    # API í‚¤ (í™˜ê²½ ë³€ìˆ˜ì—ì„œ ë¡œë“œ)
    load_dotenv()
    NAVER_CLIENT_ID = os.getenv("NAVER_CLIENT_ID")
    NAVER_CLIENT_SECRET = os.getenv("NAVER_CLIENT_SECRET")
    GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
    USE_GEMINI = os.getenv("USE_GEMINI", "1") == "1" and bool(GOOGLE_API_KEY)
    
    # (ì‚­ì œ) ì¹´ì¹´ì˜¤í†¡ API í‚¤ ë¶€ë¶„ ì œê±°
    
    # ì¤‘ë³µ ì œê±° ë°©ì‹ ì„ íƒ ("KEYWORD" ë˜ëŠ” "GEMINI")
    DEDUPE_METHOD = os.getenv("DEDUPE_METHOD", "KEYWORD").upper()

    # ìš”ì²­ í—¤ë”
    USER_AGENT = "KR-RE-NEWS/" + VERSION
    REQ_HEADERS = {"User-Agent": USER_AGENT, "Accept-Language": "ko-KR,ko;q=0.9"}

    # í—ˆìš© ì–¸ë¡ ì‚¬
    WHITELIST = {
        "ë§¤ì¼ê²½ì œ","í•œêµ­ê²½ì œ","ì„œìš¸ê²½ì œ","ì¡°ì„ ì¼ë³´","ì¤‘ì•™ì¼ë³´","ë™ì•„ì¼ë³´","í•œê²¨ë ˆ","ê²½í–¥ì‹ ë¬¸",
        "ì—°í•©ë‰´ìŠ¤","ë‰´ì‹œìŠ¤","ì¡°ì„ ë¹„ì¦ˆ","ë¨¸ë‹ˆíˆ¬ë°ì´","íŒŒì´ë‚¸ì…œë‰´ìŠ¤","KBS","SBS","MBC","ì •ì±…ë¸Œë¦¬í•‘"
    }
    DOMAIN2OUTLET = {
        "mk.co.kr":"ë§¤ì¼ê²½ì œ","hankyung.com":"í•œêµ­ê²½ì œ","sedaily.com":"ì„œìš¸ê²½ì œ",
        "chosun.com":"ì¡°ì„ ì¼ë³´","joongang.co.kr":"ì¤‘ì•™ì¼ë³´","donga.com":"ë™ì•„ì¼ë³´",
        "hani.co.kr":"í•œê²¨ë ˆ","khan.co.kr":"ê²½í–¥ì‹ ë¬¸","yna.co.kr":"ì—°í•©ë‰´ìŠ¤",
        "newsis.com":"ë‰´ì‹œìŠ¤","biz.chosun.com":"ì¡°ì„ ë¹„ì¦ˆ","mt.co.kr":"ë¨¸ë‹ˆíˆ¬ë°ì´",
        "fnnews.com":"íŒŒì´ë‚¸ì…œë‰´ìŠ¤","news.kbs.co.kr":"KBS","news.sbs.co.kr":"SBS",
        "imnews.imbc.com":"MBC","korea.kr":"ì •ì±…ë¸Œë¦¬í•‘"
    }

    # ==============================================================================
    # (ìˆ˜ì •) í‚¤ì›Œë“œ ë° í•„í„°ë§ ê·œì¹™
    # ==============================================================================

    # 1. 10ê°œ ì¹´í…Œê³ ë¦¬ë³„ ì„¸ë¶€ í‚¤ì›Œë“œ (í•„í„°ë§/ê°€ì¤‘ì¹˜ìš©)
    MARKET_KWS = ["ì•„íŒŒíŠ¸ê°’", "ì§‘ê°’", "ë§¤ë§¤ê°€", "ì „ì„¸ê°€", "ìƒìŠ¹", "í•˜ë½", "ë³´í•©", "ê¸‰ë“±", "ê¸‰ë½", "ë°˜ë“±", "ê±°ë˜ëŸ‰", "ê±°ë˜ì ˆë²½", "ë§¤ë¬¼ ì ì²´", "ë§¤ìˆ˜ ì‹¬ë¦¬", "ê´€ë§ì„¸", "ê°­íˆ¬ì", "ì™¸ì§€ì¸ íˆ¬ì", "ë‚™ì°°ê°€ìœ¨", "ê²½ë§¤"]
    POLICY_KWS = ["ë¶€ë™ì‚° ëŒ€ì±…", "ì£¼ê±° ì•ˆì •", "1.10 ëŒ€ì±…", "ì„ëŒ€ì°¨ 3ë²•", "ì „ì›”ì„¸ ìƒí•œì œ", "ê³„ì•½ê°±ì‹ ì²­êµ¬ê¶Œ", "ì£¼ê±° ì‚¬ë‹¤ë¦¬", "ì²­ë…„ ì£¼ê±°", "ì‹ í˜¼ë¶€ë¶€ ì§€ì›", "ê³µì‹œê°€ê²© í˜„ì‹¤í™”", "êµ­í† êµí†µë¶€", "ê¸°íšì¬ì •ë¶€", "ê¸ˆìœµìœ„ì›íšŒ", "í•œêµ­ì€í–‰"]
    REGULATION_KWS = ["ê·œì œ ì™„í™”", "ê·œì œ ê°•í™”", "íˆ¬ê¸°ê³¼ì—´ì§€êµ¬", "ì¡°ì •ëŒ€ìƒì§€ì—­", "ê·œì œì§€ì—­ í•´ì œ", "ë¶„ì–‘ê°€ ìƒí•œì œ", "ë¶„ìƒì œ", "ì‹¤ê±°ì£¼ ì˜ë¬´", "í† ì§€ê±°ë˜í—ˆê°€êµ¬ì—­", "ìê¸ˆì¡°ë‹¬ê³„íšì„œ"]
    FINANCE_KWS = ["ê¸°ì¤€ê¸ˆë¦¬", "ê¸ˆë¦¬ ì¸ìƒ", "ê¸ˆë¦¬ ì¸í•˜", "ì½”í”½ìŠ¤", "COFIX", "ê°€ê³„ë¶€ì±„", "PF", "í”„ë¡œì íŠ¸ íŒŒì´ë‚¸ì‹±", "ë¶€ì‹¤", "ì—°ì²´ìœ¨", "ì£¼íƒê¸ˆìœµê³µì‚¬", "HF", "HUG", "ì£¼íƒì—°ê¸ˆ"]
    LOAN_KWS = ["LTV", "DTI", "DSR", "ìŠ¤íŠ¸ë ˆìŠ¤ DSR", "ì£¼íƒë‹´ë³´ëŒ€ì¶œ", "ì£¼ë‹´ëŒ€", "ì „ì„¸ìê¸ˆëŒ€ì¶œ", "ë””ë”¤ëŒëŒ€ì¶œ", "ë³´ê¸ˆìë¦¬ë¡ ", "íŠ¹ë¡€ë³´ê¸ˆìë¦¬ë¡ ", "ì‹ ìƒì•„ íŠ¹ë¡€ëŒ€ì¶œ", "ëŒ€ì¶œ í•œë„", "ê°€ì‚°ê¸ˆë¦¬", "ì¤‘ë„ê¸ˆ ëŒ€ì¶œ"]
    TAX_KWS = ["ì–‘ë„ì„¸", "ì·¨ë“ì„¸", "ì¢…ë¶€ì„¸", "ì¬ì‚°ì„¸", "ì¦ì—¬ì„¸", "ìƒì†ì„¸", "ë¹„ê³¼ì„¸", "ê°ë©´", "ì¤‘ê³¼", "ìœ ì˜ˆ", "ì–‘ë„ì„¸ ì¤‘ê³¼ ìœ ì˜ˆ", "1ê°€êµ¬ 1ì£¼íƒ", "ë‹¤ì£¼íƒì", "ì¼ì‹œì  2ì£¼íƒ", "ë¶„ì–‘ê¶Œ ì„¸ê¸ˆ", "ì„¸ë²• ê°œì •"]
    SUPPLY_KWS = ["ê³µê¸‰ ëŒ€ì±…", "ì…ì£¼ ë¬¼ëŸ‰", "ê³µê¸‰ ê°€ë­„", "ê³µê³µë¶„ì–‘", "ë¯¼ê°„ë¶„ì–‘", "ì‚¬ì „ ì²­ì•½", "ê³µê³µì„ëŒ€", "3ê¸° ì‹ ë„ì‹œ", "ì‹ ë„ì‹œ", "íƒì§€ì§€êµ¬", "LH", "ì¸í—ˆê°€"]
    DEVELOPMENT_KWS = ["ì¬ê°œë°œ", "ì¬ê±´ì¶•", "ë¦¬ëª¨ë¸ë§", "ì‹ ì†í†µí•©ê¸°íš", "ì‹ í†µê¸°íš", "ëª¨ì•„íƒ€ìš´", "ì—­ì„¸ê¶Œ ê°œë°œ", "ì¬ê±´ì¶•ì´ˆê³¼ì´ìµí™˜ìˆ˜ì œ", "ì¬ì´ˆí™˜", "ì•ˆì „ì§„ë‹¨", "GTX", "ì‹ ì„¤ì—­", "SOC", "ì •ë¹„ì‚¬ì—…"]
    SUBSCRIPTION_KWS = ["ì²­ì•½", "ì²­ì•½ ê²½ìŸë¥ ", "ì¤ì¤", "ë¬´ìˆœìœ„ ì²­ì•½", "ë¯¸ê³„ì•½", "ì²­ì•½ ê°€ì ", "ë‹¹ì²¨ì„ ", "ë¯¸ë¶„ì–‘", "ì•…ì„± ë¯¸ë¶„ì–‘", "ì™„íŒ", "ë¶„ì–‘ê°€", "ëª¨ë¸í•˜ìš°ìŠ¤", "ê²¬ë³¸ì£¼íƒ"]
    BROKERAGE_KWS = ["ê³µì¸ì¤‘ê°œì‚¬ë²•", "ì¤‘ê°œë³´ìˆ˜", "ìˆ˜ìˆ˜ë£Œ", "ì‹¤ê±°ë˜ê°€", "ë¶€ë™ì‚° í”Œë«í¼", "í—ˆìœ„ ë§¤ë¬¼", "ì „ìê³„ì•½", "ì „ì„¸ ì‚¬ê¸°", "ì´ì¤‘ê³„ì•½", "í™•ì¸ì„¤ëª…ì„œ", "ì¤‘ê°œì—…ì†Œ"]

    # 2. í•µì‹¬ ê²€ìƒ‰ ë° í•„í„°ë§ ë¦¬ìŠ¤íŠ¸
    
    # 2-0. (*** ìˆ˜ì • ***) ì œëª© 1ì°¨ í•„í„°ë§ìš© í•µì‹¬ ëª…ì‚¬ (í¬ê´„ì  ë‹¨ì–´ ì¶”ê°€)
    # Naver ê²€ìƒ‰ ì§í›„ 1ì°¨ í•„í„°ë§ì— ì‚¬ìš©ë¨. (has_core_in_title)
    CORE_IN_TITLE = ["ë¶€ë™ì‚°", "ì£¼íƒ", "ì•„íŒŒíŠ¸", "ì²­ì•½", "ì‹œì¥", "ì£¼ê±°", # <-- 5ê°œ ëª» ì±„ìš°ëŠ” ë¬¸ì œ í•´ê²°ìš© í¬ê´„ í‚¤ì›Œë“œ
                     "ì§‘ê°’","ì•„íŒŒíŠ¸ê°’","ë§¤ë§¤ê°€ê²©","ì „ì„¸ê°€ê²©","ì „ì…‹ê°’","ê°€ê²©ì§€ìˆ˜","KBì‹œì„¸","í•œêµ­ë¶€ë™ì‚°ì›",
                     "ê±°ë˜ëŸ‰","ê±°ë˜ì ˆë²½","ë§¤ë¬¼","ìˆ˜ê¸‰","ê³µê¸‰","ì…ì£¼ë¬¼ëŸ‰","ë¶„ì–‘ë¬¼ëŸ‰","ë¯¸ë¶„ì–‘",
                     "ëŒ€ì±…","ê³µê¸‰ëŒ€ì±…","ê·œì œì§€ì—­","í† ì§€ê±°ë˜í—ˆê°€","ì •ë¹„ì‚¬ì—…","ì¬ê±´ì¶•","ì¬ê°œë°œ",
                     "ê¸ˆë¦¬","ê¸°ì¤€ê¸ˆë¦¬","LTV","DSR","ëŒ€ì¶œê·œì œ","ì „ì„¸ëŒ€ì¶œ","ë³´ìœ ì„¸","ì¢…ë¶€ì„¸","ì·¨ë“ì„¸","ì–‘ë„ì„¸"]

    # 2-1. (ëŒ€ì²´) ì „ì²´ ë¶€ë™ì‚° í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ (ëª¨ë“  ì¹´í…Œê³ ë¦¬ í†µí•©)
    # ë³¸ë¬¸ ë‚´ìš© ì²´í¬(body_is_real_estate)ì— ì‚¬ìš©ë¨. ê¸°ì¡´ REAL_ESTATE_KWS ëŒ€ì²´.
    ALL_KWS = list(set(
        MARKET_KWS + POLICY_KWS + REGULATION_KWS + FINANCE_KWS + LOAN_KWS +
        TAX_KWS + SUPPLY_KWS + DEVELOPMENT_KWS + SUBSCRIPTION_KWS + BROKERAGE_KWS +
        CORE_IN_TITLE # 1ì°¨ í•„í„° í‚¤ì›Œë“œë„ ë‹¹ì—°íˆ í¬í•¨
    ))

    # 2-2. (ìˆ˜ì •) í•µì‹¬ ê²€ìƒ‰ì–´ ë¦¬ìŠ¤íŠ¸ (ê¸°ì‚¬ ìˆ˜ì§‘ìš©)
    QUERY_TERMS = [
        "ì•„íŒŒíŠ¸ê°’", "ì „ì„¸ê°€", "ë¶€ë™ì‚° ê±°ë˜ëŸ‰", "ë¯¸ë¶„ì–‘", "ì…ì£¼ë¬¼ëŸ‰",                 # ì‹œì¥
        "ë¶€ë™ì‚° ëŒ€ì±…", "ê³µê¸‰ ëŒ€ì±…", "êµ­í† ë¶€ ë°œí‘œ", "ì„¸ë²• ê°œì •",                       # ì •ì±…/ì„¸ê¸ˆ
        "ê·œì œì§€ì—­ í•´ì œ", "í† ì§€ê±°ë˜í—ˆê°€êµ¬ì—­", "ë¶„ì–‘ê°€ ìƒí•œì œ", "ì‹¤ê±°ì£¼ ì˜ë¬´",           # ê·œì œ
        "GTX", "ì¬ê±´ì¶•", "ì¬ê°œë°œ", "ì‹ í†µê¸°íš", "ëª¨ì•„íƒ€ìš´",                           # ê°œë°œ
        "LTV", "DSR", "ìŠ¤íŠ¸ë ˆìŠ¤ DSR", "ì£¼íƒë‹´ë³´ëŒ€ì¶œ", "ì‹ ìƒì•„ íŠ¹ë¡€ëŒ€ì¶œ", "ì „ì„¸ì‚¬ê¸°",  # ê¸ˆìœµ/ëŒ€ì¶œ
        "ì²­ì•½ ê²½ìŸë¥ ", "ì¤ì¤", "ì‚¬ì „ ì²­ì•½",                                           # ë¶„ì–‘
        "í•œêµ­ë¶€ë™ì‚°ì›", "KBì‹œì„¸"                                                     # ì§€í‘œ
    ]
    
    # 2-3. (ìˆ˜ì •) ë¸”ë™ë¦¬ìŠ¤íŠ¸ (ê¸°ì‚¬ ì œì™¸)
    BLACKLIST = [
        # ê¸°ì¡´ ë¦¬ìŠ¤íŠ¸
        "ì‚¬ì„¤", "ì¹¼ëŸ¼", "opinion", "ê¸°ê³ ", "ë§Œí‰", "ìƒë‹´", "ì—°ì˜ˆ", "ê²Œì„", "ìŠ¤í¬ì¸ ",
        "í™”ì¬", "í­ë°œ", "ì‚¬ê³ ", "ì²´ë‚©", "íš¡ë ¹", "ì²´í¬", "ETF", "í€ë“œ", "ì£¼ì‹",
        "ì±„ê¶Œ", "ì„ ë¬¼", "ì˜µì…˜", "ì½”ì¸", "ë¹„íŠ¸ì½”ì¸", "ì›¹3", "ê°€ìƒìì‚°",
        # ì¶”ê°€ ë¦¬ìŠ¤íŠ¸
        "ë¶€ê³ ", "ì¸ì‚¬", "ë™ì •", "ìš´ì„¸", "í¬í† ", "ë‚ ì”¨", "ì¦ì‹œ", "í™˜ìœ¨",           # ë¹„ê´€ë ¨ ì„¹ì…˜
        "ì´ë²¤íŠ¸", "ê²½í’ˆ", "ì¶”ì²¨", "ë¬´ë£Œ", "í• ì¸", "ê´‘ê³ ", "í™ë³´", "í˜‘ì°¬",           # ê´‘ê³ /ìŠ¤íŒ¸
        "OOO ê¸°ì" # ê°€ë” ê¸°ì ì´ë¦„ì´ ì œëª©ì— í¬í•¨ë  ë•Œ
    ]

    # 2-4. (ìˆ˜ì •) ê¸°ì‚¬ ì œëª© í˜ë„í‹° í‚¤ì›Œë“œ (ì¤‘ìš”ë„ DOWN)
    TITLE_PENALTY = [
        "ì„¤ë¬¸", "ì „ë§", "ì˜ˆìƒ", "ì˜ˆì¸¡", "ì¸í„°ë·°", "ë¶„ì„", "ì˜ê²¬", "ì „ë¬¸ê°€", "ê´€ê³„ì", # ì¶”ì¸¡/ì˜ê²¬
        "í™”ì œ", "ëˆˆê¸¸", "ì´ìœ ëŠ”", " ì‚´í´ë³´ë‹ˆ",                                 # í™ë³´/ê°€ì‹­
        "ì˜¤í”¼ìŠ¤í…”", "ìƒê°€", "ì§€ì‹ì‚°ì—…ì„¼í„°", "ë¹Œë¼", "ìƒí™œí˜•ìˆ™ë°•ì‹œì„¤",              # (ì•„íŒŒíŠ¸ ì™¸)
        "í•´ì™¸", "ë¯¸êµ­", "ì¤‘êµ­"                                                  # í•´ì™¸ ë¶€ë™ì‚°
    ]

    # 2-5. (ìˆ˜ì •) ê¸°ì‚¬ ì œëª© ë³´ë„ˆìŠ¤ í‚¤ì›Œë“œ (ì¤‘ìš”ë„ UP)
    TITLE_BONUS = [
        "ë°œí‘œ", "ì‹œí–‰", "í™•ëŒ€", "ì™„í™”", "ê°•í™”", "ë„ì…", "íì§€", "ìœ ì˜ˆ", "ê°œì •",    # ì •ì±…/ê·œì œ ë™ì‚¬
        "ê³µê¸‰", "ëŒ€ì±…", "ê·œì œ", "ì„¸ì œ", "ëŒ€ì¶œ", "íŠ¹ë¡€", "LTV", "DSR", "ì¬ì´ˆí™˜", # í•µì‹¬ ì •ì±…
        "GTX", "ì¬ê±´ì¶•", "ì¬ê°œë°œ", "ì‹ í†µê¸°íš", "ëª¨ì•„íƒ€ìš´", "ì•ˆì „ì§„ë‹¨", "ì¸í—ˆê°€",   # í•µì‹¬ ê°œë°œ
        "ë¯¸ë¶„ì–‘", "ì‹¤ê±°ë˜", "ê±°ë˜ëŸ‰", "ê¸‰ë“±", "ê¸‰ë½", "LH", "HUG", "êµ­í† ë¶€"       # í•µì‹¬ í˜„ìƒ/ê¸°ê´€
    ]
    # ==============================================================================

def make_session():
    s = requests.Session()
    retry = Retry(total=3, backoff_factor=0.5, status_forcelist=(429, 500, 502, 503, 504), allowed_methods=frozenset(["GET", "POST"]))
    adapter = HTTPAdapter(max_retries=retry)
    s.mount("http://", adapter)
    s.mount("https://", adapter)
    s.headers.update(Config.REQ_HEADERS)
    return s
SESSION = make_session()

# â”€â”€ (ì‚­ì œ) KakaoTalk Functions ì„¹ì…˜ ì „ì²´ ì œê±° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# â”€â”€ (ìœ ì§€) Email Function â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def send_email_report(html_content: str, subject: str):
    """ì§€ì •ëœ ì´ë©”ì¼ë¡œ HTML ë¦¬í¬íŠ¸ë¥¼ ë°œì†¡í•©ë‹ˆë‹¤."""
    
    sender_email = os.getenv("EMAIL_USER")
    receiver_email = os.getenv("EMAIL_RECEIVER")
    app_password = os.getenv("EMAIL_PASSWORD")

    if not all([sender_email, receiver_email, app_password]):
        logging.warning("ì´ë©”ì¼ í™˜ê²½ë³€ìˆ˜(USER, PASSWORD, RECEIVER)ê°€ ì—†ì–´ ë°œì†¡ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return

    # --- SMTP ì„œë²„ ì„¤ì • (Gmail / Naver) ---
    if "@gmail.com" in sender_email:
        smtp_server = "smtp.gmail.com"
        smtp_port = 587
    elif "@naver.com" in sender_email:
        smtp_server = "smtp.naver.com"
        smtp_port = 587 # ë˜ëŠ” 465 (SSL)
    else:
        logging.error("ì§€ì›ë˜ì§€ ì•ŠëŠ” ì´ë©”ì¼ ë„ë©”ì¸ì…ë‹ˆë‹¤. (Gmail/Naverë§Œ ìë™ ì§€ì›)")
        return
    
    try:
        # --- ë©”ì‹œì§€ ë³¸ë¬¸ êµ¬ì„± ---
        msg = MIMEMultipart()
        msg['Subject'] = subject
        msg['From'] = sender_email
        msg['To'] = receiver_email
        msg.attach(MIMEText(html_content, 'html', 'utf-8'))

        # --- ì„œë²„ ì—°ê²° ë° ë°œì†¡ ---
        logging.info(f"{smtp_server}ì— ì—°ê²°í•˜ì—¬ ì´ë©”ì¼ ë°œì†¡ì„ ì‹œë„í•©ë‹ˆë‹¤...")
        with smtplib.SMTP(smtp_server, smtp_port) as server:
            server.starttls()  # TLS ì•”í˜¸í™”
            server.login(sender_email, app_password)
            server.sendmail(sender_email, receiver_email, msg.as_string())
        
        logging.info("ì´ë©”ì¼ ì „ì†¡ ì„±ê³µ!")
    
    except Exception as e:
        logging.error(f"ì´ë©”ì¼ ì „ì†¡ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

# â”€â”€ Gemini (Google Generative AI) & Deduplication â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
gem_model = None
if Config.USE_GEMINI:
    try:
        import absl.logging
        absl.logging.set_verbosity(absl.logging.ERROR)
        import google.generativeai as genai
        genai.configure(api_key=Config.GOOGLE_API_KEY)
        gem_model = genai.GenerativeModel("gemini-pro-latest")
    except Exception as e:
        logging.warning(f"Gemini ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        Config.USE_GEMINI = False
        gem_model = None

def ai_summarize_gemini(text: str) -> str:
    if not gem_model or not text: return ""
    
    # ì§€ì‹œì‚¬í•­ê³¼ ì‹¤ì œ í…ìŠ¤íŠ¸(text)ë¥¼ f-stringìœ¼ë¡œ ê²°í•©
    full_prompt = f"""ë‹¤ìŒ í•œêµ­ì–´ ë‰´ìŠ¤ ë³¸ë¬¸ì„ 3~5ê°œì˜ ì™„ë²½í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ì¤˜. ê° ë¬¸ì¥ì€ 'ë‹¤'ë‚˜ 'ìš”'ë¡œ ëë‚˜ì•¼ í•´. í•µì‹¬ ë°ì´í„°, ì •ì±…, ì‹œì¥ ë™í–¥ì„ í¬í•¨í•˜ê³  ì‚¬ì‹¤ ì¤‘ì‹¬ìœ¼ë¡œ ì‘ì„±í•´ì¤˜. ì„œë¡ ì´ë‚˜ ë¶€ì—° ì„¤ëª… ì—†ì´ ìš”ì•½ ë¬¸ì¥ìœ¼ë¡œ ë°”ë¡œ ì‹œì‘í•´ì¤˜.

[ë‰´ìŠ¤ ë³¸ë¬¸]
{text}
"""
    
    try:
        # ìˆ˜ì •: 'prompt' ëŒ€ì‹  'full_prompt'ë¥¼ ì „ë‹¬
        resp = gem_model.generate_content(full_prompt)  
        out = (resp.text or "").strip()
        
        # í›„ì²˜ë¦¬ ë¡œì§ì€ ê¸°ì¡´ê³¼ ë™ì¼
        sents = re.findall(r'[^.!?]+(?:[ë‹¤ìš”]\.|[.!?])', out)
        sents = [s.strip() for s in sents if s.strip()]
        return " ".join(sents)
    
    except Exception as e:
        logging.error(f"Gemini ìš”ì•½ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return ""

def are_summaries_similar_by_keyword(s1: str, s2: str, thr: float = 0.4) -> bool:
    stop = {"ì„œìš¸","ì§€ì—­","ì •ë¶€","ì‹œì¥","ê²½ì œ","ê¸°ì","ì •ì±…","ë¶€ë™ì‚°","ì•„íŒŒíŠ¸","ê´€ë ¨","ìœ„í•´","ëŒ€í•œ","ë”°ë¥´ë©´","ë°í˜”ë‹¤","ì „ë§","ì˜ˆìƒ","ì˜¬í•´","ë‚´ë…„"}
    def tokenize(s):
        return {w for w in re.sub(r"['\"â€œâ€â€˜â€™\[\]\(\)\.]", " ", s.lower()).split() if len(w) > 1 and w not in stop}
    toks1, toks2 = tokenize(s1), tokenize(s2)
    if not toks1 or not toks2: return False
    return (len(toks1 & toks2) / len(toks1 | toks2)) >= thr

def filter_diverse_articles_by_keyword(articles, target=5, max_per_outlet=2):
    results = []
    outlet_count = {}
    for article in articles:
        if len(results) >= target: break
        if outlet_count.get(article["outlet"], 0) >= max_per_outlet: continue
        is_too_similar = any(are_summaries_similar_by_keyword(article['summary'], ex['summary']) for ex in results)
        if not is_too_similar:
            results.append(article)
            outlet_count[article["outlet"]] = outlet_count.get(article["outlet"], 0) + 1
    return results

def filter_diverse_articles_by_gemini(articles, target=5):
    if not gem_model:
        logging.warning("Gemini ëª¨ë¸ì´ ì—†ì–´ í‚¤ì›Œë“œ ê¸°ë°˜ ì¤‘ë³µ ì œê±°ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.")
        return filter_diverse_articles_by_keyword(articles, target)

    logging.info("Geminië¥¼ ì´ìš©í•œ ì£¼ì œ ê·¸ë£¹í•‘ ë° ëŒ€í‘œ ê¸°ì‚¬ ì„ ì •ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    
    # AIì—ê²Œ ì „ë‹¬í•  í›„ë³´êµ°ì„ 15ê°œë¡œ ì œí•œí•˜ì—¬ ë¹„ìš© ë° ì‹œê°„ íš¨ìœ¨í™”
    candidate_articles = articles[:15]
    
    article_candidates_str = ""
    for i, article in enumerate(candidate_articles):
        article_candidates_str += f"ID: {i}\n"
        article_candidates_str += f"ì œëª©: {article['title']}\n"
        article_candidates_str += f"ìš”ì•½: {article['summary']}\n---\n"

    prompt = f"""ë‹¹ì‹ ì€ í•œêµ­ ë¶€ë™ì‚° ë‰´ìŠ¤ ì „ë¬¸ í¸ì§‘ì¥ì…ë‹ˆë‹¤. ì•„ë˜ëŠ” ì˜¤ëŠ˜ ìˆ˜ì§‘ëœ ì—¬ëŸ¬ ë‰´ìŠ¤ ê¸°ì‚¬ì˜ ìš”ì•½ë¬¸ ëª©ë¡ì…ë‹ˆë‹¤.
    
    [ì„ë¬´]
    1. ëª¨ë“  ê¸°ì‚¬ë¥¼ ì½ê³  ë‚´ìš©ì´ ì˜ë¯¸ì ìœ¼ë¡œ ê°™ì€ í•µì‹¬ ì‚¬ê±´(event)ì´ë‚˜ ì£¼ì œë¥¼ ë‹¤ë£¨ëŠ” ê²ƒë¼ë¦¬ ê·¸ë£¹ìœ¼ë¡œ ë¬¶ì–´ì£¼ì„¸ìš”.
    2. ê° ê·¸ë£¹ì—ì„œ ê°€ì¥ ë‚´ìš©ì„ í¬ê´„ì ì´ê³  ì˜ ì„¤ëª…í•˜ëŠ” ëŒ€í‘œ ê¸°ì‚¬ 'ID'ë¥¼ í•˜ë‚˜ì”©ë§Œ ì„ íƒí•´ì£¼ì„¸ìš”.
    3. ìµœì¢…ì ìœ¼ë¡œ ì„ íƒëœ ëŒ€í‘œ ê¸°ì‚¬ë“¤ì˜ 'ID'ë¥¼ ì‰¼í‘œ(,)ë¡œ êµ¬ë¶„í•˜ì—¬ {target}ê°œë§Œ ì•Œë ¤ì£¼ì„¸ìš”. ë‹¤ë¥¸ ì„¤ëª…ì€ ì ˆëŒ€ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”.

    [ê¸°ì‚¬ ëª©ë¡]
    {article_candidates_str}

    [ì¶œë ¥ í˜•ì‹]
    ID1,ID2,ID3,ID4,ID5
    """
    
    try:
        resp = gem_model.generate_content(prompt)
        selected_ids_str = (resp.text or "").strip()
        selected_ids = [int(id_str.strip()) for id_str in selected_ids_str.split(',') if id_str.strip().isdigit()]
        
        if not selected_ids:
            raise ValueError("Geminiê°€ ìœ íš¨í•œ IDë¥¼ ë°˜í™˜í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        logging.info(f"Geminiê°€ ì„ ì •í•œ ëŒ€í‘œ ê¸°ì‚¬ ID: {selected_ids}")
        
        results = [candidate_articles[i] for i in selected_ids if i < len(candidate_articles)]
        return results[:target]

    except Exception as e:
        logging.error(f"Gemini ê·¸ë£¹í•‘ ì‹¤íŒ¨, í‚¤ì›Œë“œ ë°©ì‹ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤: {e}")
        return filter_diverse_articles_by_keyword(articles, target)

# â”€â”€ Helper, Fetching, Parsing, Reranking Functions â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def host(url:str) -> str:
    try: return urlparse(url).netloc.lower()
    except: return ""
def norm_link(u:str) -> str:
    try: return urlunparse((urlparse(u).scheme, urlparse(u).netloc, urlparse(u).path, "", "", ""))
    except: return u or ""
def normalize_news_url(u:str)->str:
    return re.sub(r"\?.*$", "", norm_link(u))
def looks_like_article_url(u:str)->bool:
    return any(re.search(p, u) for p in [r"sedaily\.com", r"hankyung\.com", r"yna\.co\.kr", r"sbs\.co.kr", r"chosun\.com", r"mk\.co.kr", r"mt\.co.kr", r"fnnews\.com", r"newsis\.com", r"korea\.kr", r"joongang\.co.kr", r"hani\.co\.kr", "khan\.co\.kr", r"kbs\.co\.kr", r"imbc\.com"])
def outlet_from_url(url:str) -> str:
    h = host(url)
    for dom, name in Config.DOMAIN2OUTLET.items():
        if dom in h: return name
    return ""
def norm_title(t:str) -> str:
    # 1. ì•/ë’¤ ê³µë°± ë° ì—¬ëŸ¬ ê³µë°±ì„ í•˜ë‚˜ë¡œ í•©ì¹¨
    t = re.sub(r"\s+", " ", t).strip()
    
    # 2. [ì†ë³´], [ë‹¨ë…] ê°™ì€ ì ‘ë‘ì‚¬ ì œê±° (ê¸°ì¡´ ë¡œì§)
    t = re.sub(r"^\[[^\]]+\]\s*", "", t)
    
    # 3. (ì¶”ê°€) "- ì–¸ë¡ ì‚¬ëª…" í˜•íƒœì˜ ì ‘ë¯¸ì‚¬ ì œê±°
    # Config.WHITELISTì— ìˆëŠ” ì–¸ë¡ ì‚¬ëª… ë¦¬ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì •ê·œì‹ ìƒì„±
    suffix_pattern = r"\s*-\s*(ë§¤ì¼ê²½ì œ|í•œêµ­ê²½ì œ|ì„œìš¸ê²½ì œ|ì¡°ì„ ì¼ë³´|ì¤‘ì•™ì¼ë³´|ë™ì•„ì¼ë³´|í•œê²¨ë ˆ|ê²½í–¥ì‹ ë¬¸|ì—°í•©ë‰´ìŠ¤|ë‰´ì‹œìŠ¤|ì¡°ì„ ë¹„ì¦ˆ|ë¨¸ë‹ˆíˆ¬ë°ì´|íŒŒì´ë‚¸ì…œë‰´ìŠ¤|KBS|SBS|MBC|ì •ì±…ë¸Œë¦¬í•‘)$"
    t = re.sub(suffix_pattern, "", t).strip()
    
    return t
def clean(s:str) -> str:
    return re.sub(r"\s+"," ", html.unescape(s or "")).strip()
def has_core_in_title(title:str) -> bool: return any(k in title for k in Config.CORE_IN_TICLE)
def has_blacklist(txt:str) -> bool: 
    return any(k in txt for k in Config.BLACKLIST)

# (*** ìˆ˜ì • ***) Config.REAL_ESTATE_KWS -> Config.ALL_KWSë¡œ ë³€ê²½
def body_is_real_estate(text:str, min_hits:int=2)->bool:
    return sum(1 for k in Config.ALL_KWS if k in text) >= min_hits

def naver_search(q:str, display=30, pages=3):
    headers={"X-Naver-Client-Id":Config.NAVER_CLIENT_ID, "X-Naver-Client-Secret":Config.NAVER_CLIENT_SECRET}
    items=[]
    for i in range(pages):
        start = 1 + i*display
        try:
            r = SESSION.get(Config.NAVER_ENDPOINT, headers=headers, params={"query": q, "display": display, "start": start, "sort": "date"}, timeout=10)
            r.raise_for_status()
            items.extend(r.json().get("items", []))
        except requests.exceptions.RequestException as e:
            logging.warning(f"Naver search failed for '{q}': {e}")
            break
    return items
def fetch_html(url:str) -> str:
    try:
        r = SESSION.get(url, timeout=10)
        if r.status_code == 200: return r.text
    except: pass
    return ""
def parse_page_datetime_kst(html_text:str):
    try:
        soup = BeautifulSoup(html_text, "html.parser")
        for sel in ["meta[property='article:published_time']", "meta[name='date']", "time"]:
            if n := soup.select_one(sel):
                if v:= (n.get("content") or n.get_text()):
                    dt = dtparser.parse(v.strip())
                    return dt.astimezone(Config.KST) if dt.tzinfo else Config.KST.localize(dt)
    except: pass
    return None
def extract_full_title_from_html(html_text: str) -> str:
    try:
        soup = BeautifulSoup(html_text, "html.parser")
        if og_title := soup.select_one("meta[property='og:title']"):
            return og_title.get("content", "")
        if title_tag := soup.select_one("title"):
            return title_tag.get_text("", strip=True)
    except: pass
    return ""
def resolve_naver_press_and_canonical(url:str):
    try:
        html_text = fetch_html(url)
        if not html_text: return "", url, None, None
        soup = BeautifulSoup(html_text, "html.parser")
        press = (soup.select_one(".media_end_head_top_logo img[alt]") or {}).get("alt", "").strip()
        can_url = (soup.select_one("link[rel='canonical']") or {}).get("href", url).strip()
        page_dt = parse_page_datetime_kst(html_text)
        return press, normalize_news_url(can_url), page_dt, html_text
    except: return "", url, None, None

def extract_text_from_html(url:str, html_text:str) -> str:
    try:
        extracted = trafilatura.extract(html_text, url=url)
        if extracted and len(extracted.split()) > 60: 
            return extracted.strip()
    except Exception as e:
        logging.warning(f"Trafilatura failed for {url}, falling back to BeautifulSoup: {e}")

    try:
        soup = BeautifulSoup(html_text, "html.parser")
        for sel in ["#dic_area", ".newsct_article", "article", "#articleBody", ".article_body", ".art_txt", ".article-txt", "#news_view", "#content", ".view_con", ".text_area"]:
            if node := soup.select_one(sel):
                text = node.get_text(" ", strip=True)
                if len(text.split()) > 60: 
                    return text
    except Exception as e:
        logging.warning(f"BeautifulSoup fallback failed for {url}: {e}")
    return ""

def rerank_policy_bias(items):
    def score(it):
        t = it["title"]
        sc = sum(1 for k in Config.TITLE_BONUS if k in t) - sum(0.8 for k in Config.TITLE_PENALTY if k in t)
        return sc + it["pub_kst_dt"].timestamp() * 1e-12
    return sorted(items, key=score, reverse=True)

# â”€â”€ Main Pipeline Logic â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def process_article(item: dict, since: datetime, until: datetime) -> dict | None:
    try:
        link = normalize_news_url(item.get("originallink") or item.get("link", ""))
        outlet = outlet_from_url(link)
        html_cache = None
        effective_dt = item.get('pub_kst')

        if "news.naver.com" in host(link):
            press, can_link, page_dt, html_cache = resolve_naver_press_and_canonical(link)
            if press in Config.WHITELIST:
                outlet, link = press, can_link
                if page_dt: effective_dt = page_dt
        
        if outlet not in Config.WHITELIST: return None
        if not (since <= effective_dt <= until): return None

        html_text = html_cache or fetch_html(link)
        if not html_text: return None

        body = extract_text_from_html(link, html_text)
        if not body or not body_is_real_estate(body): return None

        summary = ai_summarize_gemini(body) if Config.USE_GEMINI else item["desc"]
        full_title = extract_full_title_from_html(html_text)

        return {
            "title": norm_title(full_title or item["title"]),
            "summary": clean(summary),
            "date": effective_dt.strftime("%Y-%m-%d"),
            "outlet": outlet,
            "link": link,
            "pub_kst_dt": effective_dt,
        }
    except Exception as e:
        logging.warning(f"ê¸°ì‚¬ ì²˜ë¦¬ ì‹¤íŒ¨ ({item.get('link')}): {e}")
        return None

def main():
    since = datetime.now(Config.KST).replace(hour=0,minute=0,second=0)
    until = datetime.now(Config.KST)

    candidate_pool = []
    seen_links = set()
    for q in Config.QUERY_TERMS:
        for item in naver_search(q):
            link = normalize_news_url(item.get("originallink") or item.get("link", ""))
            if not link or link in seen_links: continue
            
            title = norm_title(clean(item.get("title","")))
            if has_blacklist(title) or not has_core_in_title(title): continue

            try:
                item['pub_kst'] = dtparser.parse(item.get("pubDate","")).astimezone(Config.KST)
                candidate_pool.append(item)
                seen_links.add(link)
            except: continue

    if not candidate_pool:
        logging.info("ìˆ˜ì§‘ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    candidate_pool.sort(key=lambda x: x['pub_kst'], reverse=True)

    processed_articles = []
    with ThreadPoolExecutor(max_workers=10) as executor:
        future_to_item = {executor.submit(process_article, item, since, until): item for item in candidate_pool[:50]}
        for future in as_completed(future_to_item):
            if processed := future.result():
                processed_articles.append(processed)

    processed_articles = rerank_policy_bias(processed_articles)
    
    # -----------------------------------------------------------------
    # (ìˆ˜ì •) 1ì°¨/2ì°¨ í•„í„°ë§ ë¡œì§
    # -----------------------------------------------------------------
    target_count = 5
    max_per_outlet = 2 # filter_diverse_articles_by_keywordì˜ ê¸°ë³¸ê°’ê³¼ ë™ì¼í•˜ê²Œ ì„¤ì •

    # --- 1ì°¨ í•„í„°ë§ (ê¸°ì¡´ ë¡œì§) ---
    if Config.DEDUPE_METHOD == "GEMINI":
        results = filter_diverse_articles_by_gemini(processed_articles, target=target_count)
    else:
        results = filter_diverse_articles_by_keyword(processed_articles, target=target_count, max_per_outlet=max_per_outlet)
    
    # --- 2ì°¨ ì±„ìš°ê¸° ë¡œì§ (ìˆ˜ì •/ì¶”ê°€) ---
    if len(results) < target_count:
        logging.info(f"1ì°¨ í•„í„°ë§ ê²°ê³¼ {len(results)}ê±´. {target_count}ê±´ì„ ì±„ìš°ê¸° ìœ„í•´ 2ì°¨ í•„í„°ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
        
        # 1ì°¨ ê²°ê³¼ì—ì„œ ì´ë¯¸ ì„ íƒëœ ê¸°ì‚¬ ë§í¬ì™€ ìš”ì•½ë¬¸
        seen_links = {r['link'] for r in results}
        seen_summaries = {r['summary'] for r in results}
        
        # 1ì°¨ ê²°ê³¼ì˜ ì–¸ë¡ ì‚¬ ì¹´ìš´íŠ¸ ì§‘ê³„
        outlet_count = {}
        for r in results:
            outlet_count[r["outlet"]] = outlet_count.get(r["outlet"], 0) + 1

        # 1ì°¨ì—ì„œ ì„ íƒë˜ì§€ ì•Šì€ ë‚˜ë¨¸ì§€ ê¸°ì‚¬ë“¤ë¡œ 2ì°¨ í•„í„°ë§
        for article in processed_articles:
            # 5ê°œ ë‹¤ ì±„ì› ìœ¼ë©´ ì¦‰ì‹œ ì¢…ë£Œ
            if len(results) >= target_count:
                break
            
            # 1ì°¨ ê²°ê³¼ì— ì´ë¯¸ í¬í•¨ëœ ê¸°ì‚¬ë©´ ê±´ë„ˆë›°ê¸°
            if article['link'] in seen_links:
                continue
                
            # 2-1. ì–¸ë¡ ì‚¬ë³„ ìµœëŒ€ ê°¯ìˆ˜ ì²´í¬
            if outlet_count.get(article["outlet"], 0) >= max_per_outlet:
                continue
                
            # 2-2. 1ì°¨ ê²°ê³¼ì™€ ë‚´ìš© ì¤‘ë³µ ì²´í¬ (í‚¤ì›Œë“œ ê¸°ë°˜)
            is_too_similar = any(are_summaries_similar_by_keyword(article['summary'], ex_summary) for ex_summary in seen_summaries)
            
            if not is_too_similar:
                results.append(article)
                seen_links.add(article['link']) # 2ì°¨ì—ì„œ ì¶”ê°€ëœ ê²ƒë„ ê¸°ë¡
                seen_summaries.add(article['summary']) # 2ì°¨ì—ì„œ ì¶”ê°€ëœ ê²ƒë„ ê¸°ë¡
                outlet_count[article["outlet"]] = outlet_count.get(article["outlet"], 0) + 1
                logging.info(f"2ì°¨ í•„í„°ë§ìœ¼ë¡œ ê¸°ì‚¬ ì¶”ê°€: [{article['outlet']}] {article['title']}")
    # -----------------------------------------------------------------

    if not results:
        logging.info("ìµœì¢… ì„ ë³„ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    # --- ê²°ê³¼ë¬¼ ìƒì„± (Text ë° HTML) ---
    subject_line = f"ğŸ“° [{since.strftime('%Y-%m-%d')}] ë¶€ë™ì‚° ë‰´ìŠ¤ ìš”ì•½ ({len(results)}ê±´)"
    
    # 1. í„°ë¯¸ë„ ì¶œë ¥ìš© (Plain Text) - (ê¸°ì¡´ê³¼ ë™ì¼)
    txt_out = [f"{subject_line} Â· {Config.VERSION}", "```text"]
    for i, r in enumerate(results, 1):
        txt_out.extend([f"{i}) ê¸°ì‚¬ì œëª©: {r['title']}", f"    ë³¸ë¬¸ ìš”ì•½: {r['summary']}", f"    ({r['date']}, {r['outlet']})\n"])
    txt_out.extend(["```", "\nê·¼ê±° ë§í¬"])
    for i, r in enumerate(results, 1):
        txt_out.append(f"- ({i}) {r['link']}")
    
    final_text_output = "\n".join(txt_out)
    print(final_text_output) # í„°ë¯¸ë„ì— ì¶œë ¥

    # 2. ì´ë©”ì¼ ë°œì†¡ìš© (HTML) - (ê¸°ì¡´ê³¼ ë™ì¼)
    
    # ìŠ¤íƒ€ì¼ì‹œíŠ¸: ì œëª©(h2) ìƒ‰ìƒ ë³€ê²½, ë§í¬ ì„¹ì…˜ ìŠ¤íƒ€ì¼ ì¶”ê°€
    styles = """
    <style>
        body { font-family: sans-serif; margin: 20px; }
        h1 { font-size: 1.3em; }
        h2 { font-size: 1.1em; margin-bottom: 5px; color: #000; } /* ë§í¬ ì œê±° í›„ ê²€ì€ìƒ‰ìœ¼ë¡œ */
        p { font-size: 0.95em; color: #333; margin-top: 5px; line-height: 1.5; }
        span { font-size: 0.85em; color: #777; }
        
        /* ê¸°ì‚¬ ë³¸ë¬¸ ì•„ì´í…œ */
        div.article-item { 
            border-bottom: 1px solid #eee; 
            padding-bottom: 15px; 
            margin-bottom: 15px; 
        }
        
        /* í•˜ë‹¨ ë§í¬ ì„¹ì…˜ */
        div.links-section {
            margin-top: 30px;
            padding-top: 15px;
        }
        div.links-section p {
            font-size: 0.9em;
            margin: 8px 0;
        }
        div.links-section a {
            color: #007bff; /* ë§í¬ ìƒ‰ìƒ */
            text-decoration: none;
        }
        div.links-section a:hover {
            text-decoration: underline;
        }
    </style>
    """
    
    html_out = [f"<html><head>{styles}</head><body>"]
    html_out.append(f"<h1>{subject_line}</h1>")
    
    # ê¸°ì‚¬ ëª©ë¡: <a> íƒœê·¸ ì œê±°
    for r in results:
        html_out.append("<div class='article-item'>")
        html_out.append(f"<h2>[{r['outlet']}] {r['title']}</h2>") # <-- <a> íƒœê·¸ ì œê±°
        html_out.append(f"<p>{r['summary']}</p>")
        html_out.append(f"<span>({r['date']})</span>")
        html_out.append("</div>")
    
    # í•˜ë‹¨ ê¸°ì‚¬ ë§í¬ ì„¹ì…˜
    html_out.append("<div class='links-section'>")
    html_out.append("<p><b>ê¸°ì‚¬ ë§í¬ :</b></p>")
    for i, r in enumerate(results, 1):
        # ë§í¬ë¥¼ ë²ˆí˜¸ì™€ í•¨ê»˜ <a> íƒœê·¸ë¡œ ì¶”ê°€
        html_out.append(f"<p>{i}) <a href='{r['link']}' target='_blank'>{r['link']}</a></p>")
    html_out.append("</div>")
    
    html_out.append("</body></html>")
    final_html_output = "\n".join(html_out)

    # --- (ìœ ì§€) ì´ë©”ì¼ ë°œì†¡ ---
    send_email_report(final_html_output, subject_line)
    
    # (ì‚­ì œë¨) ì¹´ì¹´ì˜¤í†¡ ë©”ì‹œì§€ ì „ì†¡ ì½”ë“œ ì—†ìŒ

if __name__ == "__main__":

    main()
